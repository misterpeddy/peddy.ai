# ALBERT: A Lite BERT For Self-Supervised Learning of Language Representations

## Overview

This paper looks to make BERT more production-friendly, by reducing the amount
of training time required and the over-all size of the network (required RAM at
training and serving time). Their contributions are two-fold: two
parameter-reduction techniques and a self-supervised loss that focuses on
modeling inter-sentence coherence. They establish new state-of-the-art results
on GLUE, RACE, and SQuAD benchmarks while having fewer parameters.

## Introduction

* Full network pre-training has led to a series of breakthroughs in language
representation learning but require large networks.
* Claim that while in the BERT paper, Devlin et al. argue that larger hidden
  size, more hidden lyaers and more attention heads always leads to better
  performance, they stop at a hidden size of 1024. They show that if this
  argument were followed and the model were scaled up to a hidden size of
  2048, it leads to model degredation (though they don't talk much about
  how much effort went into optimization of BERT-xlarge relative to
  BERT-large for the task).
* The existing work mostly look at model parallelization and clever memory
  management to address memory limitation issues, but not much about the
  communication overhead (training) and model degredation (problem described
  in the previous bullet)
* Parameter reduction technique #1: factorized embedding parameterization -
  separate the embedding matrix so that the size of the embedding vocabulary
  is independent from that of the hidden layer.
* #2: cross-layer parameter sharing so that the # of parameters doesn't grow
  with the depth of the network.
* Another contribution is the introduction of a self-supervised loss for
  sentence-order prediction (SOP) which addresses some problems with the
  next-sentence prediction loss of the original BERT.


## Related Work

### Scaling up representation learning for NL

* Chen et al's method for checkpointing gradients to reduce the memory
  requirement to be sublinear at the cost of an extra forward pass.
* Gomez et al's method of recreating each layer's activations from the next
  layer so that intermediate activations don't need to be stored in memory.
* Makes the point that both methods trade-off speed for memory consumption and
  that their method achieves both.

### Cross-layer parameter sharing

* Sharing params across layers was explored with Transformer architecture but
  usually in the context of standard encoder-decoder models rather than the
  pretraining/finetuning setting.
* They make a point about their embeddings oscillating rather than converging
  (opposing the observations of Bai et al's DQE model in which the input
  embedding and the output embedding of a certain layer converge) which I
  don't fully follow.. Hoping that it's clarified later in the paper.

### Sentence Ordering Objectives

Most other objectives that use similar notions of inter-sentence coherence use
sentences as atoms whereas in ALBERT, the loss is defined on textual segments
rather than sentences. They claim that SOP is both a more challenging problem
and more relevant to certain downstream tasks, without citing any examples or
evidence.


## Elements of ALBERT

### Factorized embedding parameterization

Key observation is that from a modeling perspective, WordPiece embeddings (and friends)
learn context-independent representations, whereas the hidden-layer embeddings
are meant to learn context-dependent representations of the vocabulary elements.
This leads us to believe that the size of the hidden-layer embedding, $$H$$,
must be much greater than that of the WordPiece embedding ($$E$$).

Instead of directly projecting one-hot encoded vectors directly into the hidden
space of $$H$$, they first project them into a lower-dimensional embedding space of
size $$E$$ and then project it to the hidden space. By using this decomposition, they
reduce the embedding parameters from $$O(V \times H)$$ to $$O(V \times E + E \times H)$$,
which is significant because presumably $$ H \gg E $$ - it's worth noting that the former
matrix would be extremely sparsely updated during training, which is another problem that
their work avoids.

They also mention that because they use WordPiece embeddings, which are somewhat evenly
distributed across documents, they opt to use the same $$E$$ for all word pieces - a future
optimization could be varying the size of the space in which various wordpieces are embedded
in to even further save space - though that is  bounded by $$O(V \times E)$$
which may not be significant when compared to $$O(E \times H).

