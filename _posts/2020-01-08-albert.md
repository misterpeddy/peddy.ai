# ALBERT: A Lite BERT For Self-Supervised Learning of Language Representations

## Overview

This paper looks to make BERT more production-friendly, by reducing the amount
of training time required and the over-all size of the network (required RAM at
training and serving time). Their contributions are two-fold: two
parameter-reduction techniques and a self-supervised loss that focuses on
modeling inter-sentence coherence. They establish new state-of-the-art results
on GLUE, RACE, and SQuAD benchmarks while having fewer parameters.

## Introduction

* Full network pre-training has led to a series of breakthroughs in language
representation learning but require large networks.
* Claim that while in the BERT paper, Devlin et al. argue that larger hidden
  size, more hidden lyaers and more attention heads always leads to better
  performance, they stop at a hidden size of 1024. They show that if this
  argument were followed and the model were scaled up to a hidden size of
  2048, it leads to model degredation (though they don't talk much about
  how much effort went into optimization of BERT-xlarge relative to
  BERT-large for the task).
* The existing work mostly look at model parallelization and clever memory
  management to address memory limitation issues, but not much about the
  communication overhead (training) and model degredation (problem described
  in the previous bullet)
* Parameter reduction technique #1: factorized embedding parameterization -
  separate the embedding matrix so that the size of the embedding vocabulary
  is independent from that of the hidden layer.
* #2: cross-layer parameter sharing so that the # of parameters doesn't grow
  with the depth of the network.
* Another contribution is the introduction of a self-supervised loss for
  sentence-order prediction (SOP) which addresses some problems with the
  next-sentence prediction loss of the original BERT.


## Related Work

### Scaling up representation learning for NL

* Chen et al's method for checkpointing gradients to reduce the memory
  requirement to be sublinear at the cost of an extra forward pass.
* Gomez et al's method of recreating each layer's activations from the next
  layer so that intermediate activations don't need to be stored in memory.
* Makes the point that both methods trade-off speed for memory consumption and
  that their method achieves both.

### Cross-layer parameter sharing

* Sharing params across layers was explored with Transformer architecture but
  usually in the context of standard encoder-decoder models rather than the
  pretraining/finetuning setting.
* They make a point about their embeddings oscillating rather than converging
  (opposing the observations of Bai et al's DQE model in which the input
  embedding and the output embedding of a certain layer converge) which I
  don't fully follow.. Hoping that it's clarified later in the paper.

### Sentence Ordering Objectives

Most other objectives that use similar notions of inter-sentence coherence use
sentences as atoms whereas in ALBERT, the loss is defined on textual segments
rather than sentences. They claim that SOP is both a more challenging problem
and more relevant to certain downstream tasks, without citing any examples or
evidence.


## Elements of ALBERT

### Factorized embedding parameterization

Key observation is that from a modeling perspective, WordPiece embeddings (and friends)
learn context-independent representations, whereas the hidden-layer embeddings
are meant to learn context-dependent representations of the vocabulary elements.
This leads us to believe that the size of the hidden-layer embedding, $$H$$,
must be much greater than that of the WordPiece embedding ($$E$$).

Instead of directly projecting one-hot encoded vectors directly into the hidden
space of $$H$$, they first project them into a lower-dimensional embedding space of
size $$E$$ and then project it to the hidden space. By using this decomposition, they
reduce the embedding parameters from $$O(V \times H)$$ to $$O(V \times E + E \times H)$$,
which is significant because presumably $$ H \gg E $$ - it's worth noting that the former
matrix would be extremely sparsely updated during training, which is another problem that
their work avoids.

They also mention that because they use WordPiece embeddings, which are somewhat evenly
distributed across documents, they opt to use the same $$E$$ for all word pieces - a future
optimization could be varying the size of the space in which various wordpieces are embedded
in to even further save space - though that is  bounded by $$O(V \times E)$$
which may not be significant when compared to $$O(E \times H).

### Cross-layer parameter sharing

By default, ALBERT shares all parameters across layers (both for feed-forward
network as well as the attention heads). By showing that in their setup, the transitions from
layer to layer are much smoother those for BERT, they assert that weight-sharing has
an effect on stabilizing network parameters. What is note-worthy is that unlike
the DQE network mentioned in related work, neither cosine similarity nor L2 distance drop 
to 0 after 24 layers, showing that the solution space for ALBERT parameters is
likely different than the one found by DQE.

{% include image.html url="/assets/2019-01-08-albert/l2_distance_of_embedding_layers.png" description="L2
distances and cosine similarity of input and output embedding" %}

### Inter-sentence coherence loss

In addition to msaked language modeling loss, BERT uses the next-sentence
prediction loss, derived from and objective to improve performance on naturla
language inference nad reasoning about the relationship between sentence pairs.

In BERT, positive examples and negative examples (sampled with equal
probability) were created by taking consecutive segments from training corpus
for the former and pairing segments from different documents for the latter.

They claim that NSP's ineffectiveness (argued by other papers) is its lack of
diffidculty as a task, as compared to MLM, conflating **topic prediction** and
**coherence prediction** in a single task. This is due to the fact that negative
examples area not only misaligned from a coherence perspective (2 unrelated
sentences), but also from a topic perspective (2 unrelated documents). They do a
good job supporting this claim in their experimental results by showing that in
fact, NSP cannot solve the SOP task at all (compared to random).

## Experimental Results

### Setup

They follow the BERT setup fairly closely, using the BookCorpus and
English Wikipedia for pretraining baseline models. All text
si formatted as "[CLS] $$x_1$$ [SEP] $$x_2$$ [SEP]" with $$x_1 =
x_{1,1},x_{1,2}...$$ with the maximum input length set to 512, randomly
generating input sequences shorter than 512 with a probabilityh of 10%. They use
the SentencePiece tokenizer (which has a vocabulary of 30,000).

MLM target masks are generated via n-gram masking with length $$n$$ (capped at
3) given by:

$$ p(n) = \frac{1/n}{\sum{1/k}_{k=1}^N} $$

Other hyperparams include batch size of 4096, LAMB optimizer, learning rate of
0.00176 (taken directly from existing work that Yang You did late 2019 during
his internship at Brain), trained for 125,000 steps on 64-1024 Cloud TPU V3.

##3 Evaluation

